{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pymc as pm # == 4.2.0\n",
    "import scipy.stats as stats # scipy == 1.9.3\n",
    "import aesara.tensor as T \n",
    "import aesara # == 2.8.2\n",
    "import arviz as az # == 0.14.0 \n",
    "import graphviz \n",
    "aesara.config.compute_test_value='off'\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "#az.style.use(\"arviz-darkgrid\")\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that there are two random variables, $x_1$ and $x_2$, following two normal distributions, respectively:\n",
    "$$\n",
    "x_1 \\sim \\mathcal{N}(3,2^2)\\\\\n",
    "x_2 \\sim \\mathcal{N}(\\text{-}3,1^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-8,10,100), stats.norm.pdf(np.linspace(-8,10,100), loc=3, scale=2), label='first normal distribution')\n",
    "plt.plot(np.linspace(-8,10,100), stats.norm.pdf(np.linspace(-8,10,100), loc=-3, scale=1), label='second normal distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw 100 samples from each distribution. The set of 200 samples is our dataset in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # for reproducibility\n",
    "x1 = np.random.randn(100)*2+3 # first distribution, mean:3, std:2\n",
    "x2 = np.random.randn(100)*1-3 # second distribution, mean:-3, std:1\n",
    "plt.hist(x1, density=True, alpha=0.5, bins=20, range=(-8,10))\n",
    "plt.hist(x2, density=True, alpha=0.5, bins=20, range=(-8,10))\n",
    "plt.xlim(-8,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we do not know the two distributions a priori. Now, we want to fit a normal distribution (estimating $\\mu$ and $\\sigma$) to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = np.hstack((x1,x2))\n",
    "plt.hist(All, density=True, bins=20, range=(-8,10))\n",
    "plt.xlim(-8,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct our model. Again, all we want is estimating $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = pm.Model()\n",
    "with model_all:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=10) # mu\n",
    "    std = pm.Exponential('std', lam=1/10.) # sigma\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=std, observed=All) # likelihood, normal distribution\n",
    "    trace = pm.sample(draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_sample = trace.posterior['mu'][0]\n",
    "std_sample = trace.posterior['std'][0]\n",
    "\n",
    "X_plot = np.linspace(-8,10,100)\n",
    "\n",
    "pdf = np.empty((mu_sample.shape[0],100))\n",
    "for i, param in enumerate(zip(mu_sample, std_sample)):\n",
    "    #print(i, param[0].shape[0], param[1].shape[0])\n",
    "    pdf[i,:] = stats.norm.pdf(x=X_plot,loc=param[0],scale=param[1])\n",
    "pdf_med = np.median(pdf,axis=0)\n",
    "pdf_ub = np.percentile(pdf,axis=0,q=97.5)\n",
    "pdf_lb = np.percentile(pdf,axis=0,q=2.5)\n",
    "\n",
    "plt.hist(All, density=True, bins=20, range=(-8,10),zorder=0)\n",
    "plt.plot(X_plot, pdf_med,zorder=1)\n",
    "plt.fill_between(X_plot, pdf_lb, pdf_ub, alpha=0.3, color='C1',zorder=2)\n",
    "plt.xlim(-8,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we get one flat Normal distribution which is quite different from either of the original two distributions.<br>\n",
    "How can we do better?<br>\n",
    "\n",
    "From the histogram above, we can notice that fitting two normal distributions would be better than fitting one for all the datapoints.<br>\n",
    "The problem is that, we do not know which datapoint belongs to which distribution!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this probelm, we introduce a discrete variable, $z$, into our problem. We assume that, each datapoint inherently has a discrete value (either 0 or 1) that represents which distribution the datapoint belongs to. But, it's a hidden (latent/unobsevable/unmeasurable) variable.<br>\n",
    "\n",
    "Now, we estimate $z$ for each data point as well as model parameters ($\\mu_1, \\mu_2, \\sigma_1, \\sigma_2$). Let's sample $z_{1:D}$, $\\mu$'s and $\\sigma$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log likelihood, normal distribution\n",
    "def llk_norm(X, mu, tau):\n",
    "    return (-tau * (X - mu)**2. + T.log(tau / np.pi / 2.)) / 2.\n",
    "\n",
    "# log liklihood with two normal distributions.\n",
    "# if z=0, calculate the log likelihood with the first normal distribution, \n",
    "# if z=1, calculate the log likelihood with the second normal distribution.\n",
    "def llk_mixture(mu=None, std=None, z=None, X=None):\n",
    "    mu1 = mu[0]\n",
    "    std1 = std[0]\n",
    "    tau1 = 1./std1**2.\n",
    "    llk_norm1 = llk_norm(X=X, mu=mu1, tau=tau1)[:,None]\n",
    "\n",
    "    mu2 = mu[1]\n",
    "    std2 = std[1]\n",
    "    tau2 = 1./std2**2.\n",
    "    llk_norm2 = llk_norm(X=X, mu=mu2, tau=tau2)[:,None]\n",
    "    \n",
    "    llk_mixture = T.concatenate((llk_norm1, llk_norm2),axis=1)\n",
    "\n",
    "    return T.sum(llk_mixture[T.arange(X.shape[0]),z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llk_norm(5,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log likelihood, normal distribution\n",
    "def llk_norm(X, mu, tau):\n",
    "    return (-tau * (X - mu)**2. + T.log(tau / np.pi / 2.)) / 2.\n",
    "\n",
    "# log liklihood with two normal distributions.\n",
    "# if z=0, calculate the log likelihood with the first normal distribution, \n",
    "# if z=1, calculate the log likelihood with the second normal distribution.\n",
    "def llk_mixture(mu=None, std=None, z=None, X=None):\n",
    "    mu1 = mu[0]\n",
    "    std1 = std[0]\n",
    "    tau1 = 1./std1**2.\n",
    "    llk_norm1 = llk_norm(X=X, mu=mu1, tau=tau1)[:,None]\n",
    "\n",
    "    mu2 = mu[1]\n",
    "    std2 = std[1]\n",
    "    tau2 = 1./std2**2.\n",
    "    llk_norm2 = llk_norm(X=X, mu=mu2, tau=tau2)[:,None]\n",
    "    \n",
    "    llk_mixture = T.concatenate((llk_norm1, llk_norm2),axis=1)\n",
    "    \n",
    "    return T.sum(llk_mixture[T.arange(X.shape[0]),z])\n",
    "    \n",
    "model_mixture = pm.Model()\n",
    "with model_mixture:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=10, shape=2) # two mu's\n",
    "    std = pm.Exponential('std', lam=1/10., shape=2) # two sigma's\n",
    "    pi = pm.Dirichlet('pi', np.ones(2)) # you can ignore it for now\n",
    "    z = pm.Categorical('z',p=pi,shape=All.shape[0]) # z values\n",
    "    obs = pm.Potential('obs',llk_mixture(mu=mu, std=std, z=z, X=All)) # custom likelihood\n",
    "    trace = pm.sample(draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mixture = pm.Model()\n",
    "with model_mixture:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=10, shape=2) # two mu's\n",
    "    std = pm.Exponential('std', lam=1/10., shape=2) # two sigma's\n",
    "    pi = pm.Dirichlet('pi', np.ones(2)) # you can ignore it for now\n",
    "    z = pm.Categorical('z',p=pi,shape=All.shape[0]) # z values\n",
    "    obs = pm.Potential('obs',llk_mixture(mu=mu, std=std, z=z, X=All)) # custom likelihood\n",
    "    trace = pm.sample(draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_sample = trace.posteior['mu'][0,:]\n",
    "std_sample = trace.posteior['std'][0,:]\n",
    "\n",
    "idx = (mu_sample>0)\n",
    "mu_1_sample = mu_sample[idx]\n",
    "std_1_sample = std_sample[idx]\n",
    "mu_2_sample = mu_sample[~idx]\n",
    "std_2_sample = std_sample[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_1 = np.empty((mu_1_sample.shape[0],100))\n",
    "pdf_2 = np.empty((mu_2_sample.shape[0],100))\n",
    "for i,param in enumerate(zip(mu_1_sample, std_1_sample, mu_2_sample, std_2_sample)):\n",
    "    pdf_1[i,:] = stats.norm.pdf(x=X_plot,loc=param[0],scale=param[1])\n",
    "    pdf_2[i,:] = stats.norm.pdf(x=X_plot,loc=param[2],scale=param[3])\n",
    "pdf_1_med = np.median(pdf_1,axis=0)\n",
    "pdf_1_ub = np.percentile(pdf_1,axis=0,q=97.5)\n",
    "pdf_1_lb = np.percentile(pdf_1,axis=0,q=2.5)\n",
    "pdf_2_med = np.median(pdf_2,axis=0)\n",
    "pdf_2_ub = np.percentile(pdf_2,axis=0,q=97.5)\n",
    "pdf_2_lb = np.percentile(pdf_2,axis=0,q=2.5)\n",
    "\n",
    "plt.hist(x1, density=True, alpha=0.5, bins=20, range=(-8,10), color='C0')\n",
    "plt.hist(x2, density=True, alpha=0.5, bins=20, range=(-8,10), color='C0')\n",
    "plt.plot(X_plot, pdf_1_med,zorder=1,c='C1')\n",
    "plt.fill_between(X_plot, pdf_1_lb, pdf_1_ub, alpha=0.3, color='C1',zorder=2)\n",
    "plt.plot(X_plot, pdf_2_med,zorder=1,c='C2')\n",
    "plt.fill_between(X_plot, pdf_2_lb, pdf_2_ub, alpha=0.3, color='C2',zorder=2)\n",
    "plt.xlim(-8,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model discovered two normal distributions! In other words, two sub-models for two groups (clusters) of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "So, can we do similar work with our previous data? I know it will be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import / plot data\n",
    "Data3 = pd.read_excel(os.path.join(os.path.dirname('__file__'), '..', 'data', 'Data.xlsx'))\n",
    "\n",
    "plt.scatter(Data3['Area [sq ft]'], Data3['Elec [kBTU]'], facecolors='None', edgecolors='green', label='Data3')\n",
    "\n",
    "plt.xlabel('Floor Area [$\\mathrm{ft}^2$]')\n",
    "plt.ylabel('Elec. Consumption [$\\mathrm{kBTU}$]')\n",
    "plt.ylim(0.5e+8, 2.5e+8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ How can we determine the optimal number of clusters for our dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b927926d5da9f155f70e01cd763284b851c3421432ad71beda670491290494ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
